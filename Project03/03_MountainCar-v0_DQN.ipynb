{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(object):\n",
    "    def __init__(self, num_states, num_actions, update_freq, replay_size, optim_lr, epsilon):\n",
    "        self.train_steps = 0\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.update_freq = update_freq\n",
    "        self.replay_size = replay_size\n",
    "        self.optim_lr = optim_lr\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.replay_queue = deque(maxlen=self.replay_size)\n",
    "\n",
    "        self.primary_model = self.create_model()\n",
    "        self.target_model = self.create_model()\n",
    "\n",
    "    def create_model(self):\n",
    "        hidden_units = 30\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(hidden_units, \n",
    "                         input_dim = self.num_states, \n",
    "                         activation='relu', \n",
    "                         kernel_initializer='glorot_normal', \n",
    "                         bias_initializer='glorot_normal'),\n",
    "            layers.Dense(self.num_actions, \n",
    "                         activation=\"linear\",\n",
    "                         kernel_initializer='glorot_normal',\n",
    "                         bias_initializer='glorot_normal'\n",
    "                        )\n",
    "        ])\n",
    "        \n",
    "        model.compile(loss='mean_squared_error',\n",
    "                      optimizer=optimizers.Adam(self.optim_lr))\n",
    "        return model\n",
    "    \n",
    "    def epsilon_greedy(self, obs):\n",
    "        if np.random.uniform(low=0, high=1) < self.epsilon:\n",
    "            action =  np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.primary_model.predict(np.array([obs]))[0])\n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, obs, action, obs_next, reward):\n",
    "        #reward optimize\n",
    "        reward = -10 + abs(obs[0] + 0.5) + 4 * max(obs[0]-0.1,0) + 0.2 * obs[1]\n",
    "        self.replay_queue.append((obs, action, obs_next, reward))\n",
    "\n",
    "    def train(self, batch_size, alpha, gamma):\n",
    "        if len(self.replay_queue) < self.replay_size:\n",
    "            return\n",
    "        self.train_steps += 1\n",
    "\n",
    "        if self.epsilon > 0.1:\n",
    "            self.epsilon -= 0.0003\n",
    "\n",
    "        if self.train_steps % self.update_freq == 0:\n",
    "            self.target_model.set_weights(self.primary_model.get_weights())\n",
    "\n",
    "        replay_batch = random.sample(self.replay_queue, batch_size)\n",
    "        \n",
    "        obs_batch = np.array([replay[0] for replay in replay_batch])\n",
    "        obs_next_batch = np.array([replay[2] for replay in replay_batch])\n",
    "\n",
    "        Q = self.primary_model.predict(obs_batch)\n",
    "        Q_next = self.target_model.predict(obs_next_batch)\n",
    "\n",
    "        for obs, replay in enumerate(replay_batch):\n",
    "            _, act, _, reward = replay\n",
    "            Q[obs][act] = (1 - alpha) * Q[obs][act] + alpha * (reward + gamma * np.amax(Q_next[obs]))\n",
    "\n",
    "        self.primary_model.fit(obs_batch, Q, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env = env.unwrapped\n",
    "env = gym.wrappers.Monitor(env,'MountainCar-v0-DQN',force=True)\n",
    "\n",
    "num_episodes = 100\n",
    "initial_alpha = 1.0\n",
    "min_alpha = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "----------\n",
      "alpha = 1.0\n",
      "WARNING:tensorflow:From /Users/xiangyutong/opt/anaconda3/envs/env_pro03_ai/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Episode 1 completed in 3046 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 2 completed in 1593 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 3 completed in 3790 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 4 completed in 272 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 5 completed in 362 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 6 completed in 731 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 7 completed in 491 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 8 completed in 2002 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 9 completed in 977 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 10 completed in 11812 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 11 completed in 1001 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 12 completed in 5947 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 13 completed in 1332 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 14 completed in 1110 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 15 completed in 1069 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 16 completed in 1443 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 17 completed in 444 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 18 completed in 2427 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 19 completed in 440 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 20 completed in 193 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 21 completed in 306 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 22 completed in 320 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 23 completed in 212 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 24 completed in 187 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 25 completed in 257 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 26 completed in 458 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 27 completed in 196 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 28 completed in 429 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 29 completed in 293 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 30 completed in 286 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 31 completed in 435 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 32 completed in 324 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 33 completed in 273 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 34 completed in 758 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 35 completed in 313 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 36 completed in 411 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 37 completed in 472 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 38 completed in 341 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 39 completed in 478 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 40 completed in 445 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 41 completed in 1553 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 42 completed in 1486 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 43 completed in 388 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 44 completed in 464 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 45 completed in 221 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 46 completed in 283 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 47 completed in 223 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 48 completed in 243 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 49 completed in 374 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 50 completed in 507 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 51 completed in 259 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 52 completed in 392 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 53 completed in 242 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 54 completed in 230 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 55 completed in 434 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 56 completed in 895 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 57 completed in 223 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 58 completed in 311 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 59 completed in 223 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 60 completed in 372 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 61 completed in 220 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 62 completed in 303 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 63 completed in 181 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 64 completed in 760 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 65 completed in 292 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 66 completed in 207 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 67 completed in 243 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 68 completed in 138 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 69 completed in 232 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 70 completed in 251 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 71 completed in 150 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 72 completed in 223 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 73 completed in 337 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 74 completed in 431 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 75 completed in 283 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 76 completed in 374 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 77 completed in 240 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 78 completed in 278 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 79 completed in 298 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 80 completed in 1024 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 81 completed in 258 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 82 completed in 241 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 83 completed in 239 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 84 completed in 239 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 85 completed in 223 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 86 completed in 291 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 87 completed in 1096 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 88 completed in 284 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 89 completed in 219 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 90 completed in 220 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 91 completed in 367 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 92 completed in 205 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 93 completed in 215 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 94 completed in 1798 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 95 completed in 217 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 96 completed in 224 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 97 completed in 336 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 98 completed in 334 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 99 completed in 1457 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 100 completed in 359 steps\n"
     ]
    }
   ],
   "source": [
    "dqn = DQN(\n",
    "    num_states = env.observation_space.shape[0], \n",
    "    num_actions = env.action_space.n,\n",
    "    update_freq = 200, \n",
    "    replay_size = 2000,\n",
    "    optim_lr = 0.001,\n",
    "    epsilon = 0.15)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    steps = 0\n",
    "    \n",
    "    alpha = max(min_alpha, initial_alpha * (0.8 ** (episode // 5)))\n",
    "    print('----------')\n",
    "    print(\"alpha = \" + str(alpha))\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.epsilon_greedy(obs) \n",
    "        obs_next, reward, terminate, _ = env.step(action)\n",
    "        dqn.store_experience(obs, action, obs_next, reward)\n",
    "        dqn.train(\n",
    "            batch_size=64, \n",
    "            alpha=alpha, \n",
    "            gamma=0.99)\n",
    "        steps += 1\n",
    "        if terminate:\n",
    "            break\n",
    "        obs = obs_next\n",
    "    print(\"Episode {} completed in {} steps\".format(episode + 1, steps))\n",
    "\n",
    "start = time.time()\n",
    "while True: \n",
    "    env.render()\n",
    "    if (time.time()-start)>=5:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_pro03_ai]",
   "language": "python",
   "name": "conda-env-env_pro03_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
