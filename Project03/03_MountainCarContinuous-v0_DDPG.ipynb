{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import tensorflow.keras.backend as kb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, session, num_states, num_actions, actor_optim_lr, critic_optim_lr, update_freq, \n",
    "                 replay_size, batch_size, tau, gamma, theta, mu, sigma, action_high, action_low):\n",
    "        self.sess = session\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        self.actor_optim_lr = actor_optim_lr\n",
    "        self.critic_optim_lr = critic_optim_lr\n",
    "        \n",
    "        self.update_freq = update_freq\n",
    "        self.replay_size = replay_size\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.action_high = action_high\n",
    "        self.action_low = action_low\n",
    "        \n",
    "        self.train_steps = 0\n",
    "        self.replay_buffer = deque(maxlen = self.replay_size)\n",
    "        self.action_noise = np.ones(self.num_actions[0]) * self.mu\n",
    "        \n",
    "        self.actor_state_input, self.primary_actor_network = self.create_actor_network()\n",
    "        _, self.target_actor_network = self.create_actor_network()\n",
    "        self.critic_state_input, self.critic_action_input, self.primary_critic_network = self.create_critic_network()\n",
    "        _, _, self.target_critic_network = self.create_critic_network()\n",
    "        \n",
    "        self.critic_grads = tf.gradients(self.primary_critic_network.output, self.critic_action_input)\n",
    "\n",
    "        actor_network_weights = self.primary_actor_network.trainable_weights\n",
    "        self.actor_critic_grads = tf.placeholder(tf.float32,[None, self.num_actions[0]])\n",
    "\n",
    "        self.actor_grads = tf.gradients(self.primary_actor_network.output, \n",
    "                                        actor_network_weights, - self.actor_critic_grads)\n",
    "\n",
    "        self.actor_optim = tf.train.AdamOptimizer(self.actor_optim_lr).apply_gradients(\n",
    "            zip(self.actor_grads, actor_network_weights))\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def create_actor_network(self):\n",
    "        state_input = layers.Input(shape = self.num_states)\n",
    "        \n",
    "        hidden_state_1 = layers.Dense(400, \n",
    "                         activation = 'relu',\n",
    "                        )(state_input)\n",
    "        hidden_state_2 = layers.Dense(300, \n",
    "                         activation = 'relu',\n",
    "                        )(hidden_state_1)\n",
    "        output_layer = layers.Dense(self.num_actions[0], \n",
    "                         activation = 'tanh',\n",
    "                        )(hidden_state_2)\n",
    "        \n",
    "        model = models.Model(inputs=state_input, outputs=output_layer)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizers.Adam(self.actor_optim_lr))\n",
    "        \n",
    "        return state_input, model\n",
    "\n",
    "    def create_critic_network(self):\n",
    "        state_input = layers.Input(shape = self.num_states)\n",
    "        action_input = layers.Input(shape = self.num_actions)\n",
    "        \n",
    "        hidden_state_1 = layers.Dense(400,  \n",
    "                         activation = 'relu',\n",
    "                        )(state_input)\n",
    "        hidden_state_2 = layers.Dense(300, \n",
    "                         activation = None,\n",
    "                        )(hidden_state_1)\n",
    "        hidden_action_1 = layers.Dense(300, \n",
    "                         activation = None,\n",
    "                        )(action_input)\n",
    "\n",
    "        merged_layer = layers.Concatenate()([hidden_state_2, hidden_action_1])\n",
    "        \n",
    "        hidden_merged_3 = layers.Dense(300,  \n",
    "                         activation = 'relu',\n",
    "                        )(merged_layer)\n",
    "        \n",
    "        output_layer = layers.Dense(1,  \n",
    "                         activation = None,\n",
    "                        )(hidden_merged_3)\n",
    "        \n",
    "        model = models.Model(inputs=[state_input,action_input], outputs=output_layer)\n",
    "        model.compile(loss='mean_squared_error', optimizer=optimizers.Adam(self.critic_optim_lr))\n",
    "        \n",
    "        return state_input, action_input, model\n",
    "\n",
    "    def update_target_nets(self):\n",
    "        actor_primary_weights  = self.primary_actor_network.get_weights()\n",
    "        actor_target_weights = self.target_actor_network.get_weights()\n",
    "        critic_primary_weights  = self.primary_critic_network.get_weights()\n",
    "        critic_target_weights = self.target_critic_network.get_weights()\n",
    "\n",
    "        for i in range(len(actor_target_weights)):\n",
    "            actor_target_weights[i] = actor_primary_weights[i]*self.tau + actor_target_weights[i]*(1-self.tau)\n",
    "        for i in range(len(critic_target_weights)):\n",
    "            critic_target_weights[i] = critic_primary_weights[i]*self.tau + critic_target_weights[i]*(1-self.tau)\n",
    "        \n",
    "        self.target_actor_network.set_weights(actor_target_weights)\n",
    "        self.target_critic_network.set_weights(critic_target_weights)\n",
    "\n",
    "    def ou_noise(self):\n",
    "        if self.sigma < 0.01:\n",
    "            self.sigma = 0.01\n",
    "        deriv = self.theta * (self.mu - self.action_noise) + self.sigma * np.random.randn(len(self.action_noise))\n",
    "        self.action_noise += deriv\n",
    "        return self.action_noise\n",
    "    \n",
    "    def train_network(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        self.train_steps += 1\n",
    "\n",
    "        if self.train_steps % self.update_freq == 0:\n",
    "            self.update_target_nets()\n",
    "        \n",
    "        replay_batch = random.sample(self.replay_buffer,self.batch_size)\n",
    "        \n",
    "        obs_batch = np.array([replay[0] for replay in replay_batch])\n",
    "        action_batch = np.array([replay[1] for replay in replay_batch])\n",
    "        reward_batch = np.array([replay[2] for replay in replay_batch])\n",
    "        obs_next_batch = np.array([replay[3] for replay in replay_batch])\n",
    "        terminate_batch = np.array([replay[4] for replay in replay_batch])\n",
    "\n",
    "        primary_actions = self.primary_actor_network.predict(obs_batch)\n",
    "        \n",
    "        grads = self.sess.run(self.critic_grads, feed_dict={\n",
    "            self.critic_state_input:  obs_batch,\n",
    "            self.critic_action_input: primary_actions\n",
    "        })[0]\n",
    "\n",
    "        self.sess.run(self.actor_optim, feed_dict={\n",
    "            self.actor_state_input: obs_batch,\n",
    "            self.actor_critic_grads: grads\n",
    "        })\n",
    "        \n",
    "        target_actions = self.target_actor_network.predict(obs_next_batch)\n",
    "        reward_next = self.target_critic_network.predict([obs_next_batch, target_actions])\n",
    "        \n",
    "        reward_batch += self.gamma * reward_next.reshape(-1) * (1 - terminate_batch)\n",
    "\n",
    "        self.primary_critic_network.fit([obs_batch, action_batch], reward_batch, verbose=0) \n",
    "\n",
    "    def noised_action(self, obs):\n",
    "        obs = obs.reshape(1, self.num_states[0])\n",
    "        action = self.primary_actor_network.predict(obs)\n",
    "        action += self.ou_noise()\n",
    "        action = np.clip(action, self.action_low, self.action_high)\n",
    "    \n",
    "        return action\n",
    "    \n",
    "    def train_and_remember(self, obs, action, reward, obs_next, terminate):\n",
    "        reward += abs(obs[0] + 0.5) + 5 * max(obs[0],0) + 1 * obs[1]\n",
    "        self.replay_buffer.append((obs, action[0], reward, obs_next, terminate))\n",
    "        if len(self.replay_buffer) > self.replay_size:\n",
    "            self.replay_buffer.popleft()\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            self.train_network()\n",
    "        if terminate:\n",
    "            self.action_noise = np.ones(self.num_actions[0]) * self.mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "kb.set_session(session)\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env = env.unwrapped\n",
    "env = gym.wrappers.Monitor(env,'MountainCarContinuous-v0-DDPG',force=True)\n",
    "\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "num_episodes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddpg = DDPG(\n",
    "    session = session,\n",
    "    num_states = env.observation_space.shape,\n",
    "    num_actions = env.action_space.shape,\n",
    "    actor_optim_lr = 1e-4,\n",
    "    critic_optim_lr = 1e-3,\n",
    "    update_freq = 2,\n",
    "    replay_size = 10000,\n",
    "    batch_size = 64,\n",
    "    tau = 1e-3,\n",
    "    gamma = 0.99,\n",
    "    theta = 0.15,\n",
    "    mu = 0,\n",
    "    sigma = 0.2,\n",
    "    action_high = env.action_space.high,\n",
    "    action_low = env.action_space.low)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = ddpg.noised_action(obs)\n",
    "        action = action.reshape((-1,))\n",
    "        obs_next, reward, terminate, _ = env.step(action * env.action_space.high)\n",
    "        ddpg.train_and_remember(obs, action, reward, obs_next, terminate)  \n",
    "        steps += 1\n",
    "        if terminate:\n",
    "            break\n",
    "        obs = obs_next\n",
    "\n",
    "    print(\"Episode {} completed in {} steps\".format(episode + 1, steps))\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    env.render()\n",
    "    if (time.time()-start)>=5:\n",
    "        break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_pro03_ai]",
   "language": "python",
   "name": "conda-env-env_pro03_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
