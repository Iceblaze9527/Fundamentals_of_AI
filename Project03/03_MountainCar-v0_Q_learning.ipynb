{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 30\n",
    "num_episodes = 150\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 0.10\n",
    "initial_lr = 1.0\n",
    "min_lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'MountainCar-v0'\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "env = gym.wrappers.Monitor(env,'MountainCar-v0-Q-learning',force=True)\n",
    "env.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_state(env, obs):\n",
    "    env_low = env.observation_space.low\n",
    "    env_high = env.observation_space.high\n",
    "\n",
    "    env_den = (env_high - env_low) / num_states\n",
    "\n",
    "    pos_den = env_den[0]\n",
    "    vel_den = env_den[1]\n",
    "\n",
    "    pos_low = env_low[0]\n",
    "    vel_low = env_low[1]\n",
    "\n",
    "    pos_scaled = int((obs[0] - pos_low) / pos_den)\n",
    "    vel_scaled = int((obs[1] - vel_low) / vel_den)\n",
    "    \n",
    "    return pos_scaled, vel_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "alpha = 1.0\n",
      "Episode 1 completed with total reward 0 in 8218 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 2 completed with total reward 0 in 3513 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 3 completed with total reward 0 in 1259 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 4 completed with total reward 0 in 4773 steps\n",
      "----------\n",
      "alpha = 1.0\n",
      "Episode 5 completed with total reward 0 in 2647 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 6 completed with total reward 0 in 4681 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 7 completed with total reward 0 in 4769 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 8 completed with total reward 0 in 1330 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 9 completed with total reward 0 in 904 steps\n",
      "----------\n",
      "alpha = 0.8\n",
      "Episode 10 completed with total reward 0 in 569 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 11 completed with total reward 0 in 705 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 12 completed with total reward 0 in 889 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 13 completed with total reward 0 in 587 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 14 completed with total reward 0 in 819 steps\n",
      "----------\n",
      "alpha = 0.6400000000000001\n",
      "Episode 15 completed with total reward 0 in 567 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 16 completed with total reward 0 in 643 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 17 completed with total reward 0 in 786 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 18 completed with total reward 0 in 502 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 19 completed with total reward 0 in 806 steps\n",
      "----------\n",
      "alpha = 0.5120000000000001\n",
      "Episode 20 completed with total reward 0 in 520 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 21 completed with total reward 0 in 472 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 22 completed with total reward 0 in 348 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 23 completed with total reward 0 in 530 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 24 completed with total reward 0 in 471 steps\n",
      "----------\n",
      "alpha = 0.4096000000000001\n",
      "Episode 25 completed with total reward 0 in 543 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 26 completed with total reward 0 in 412 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 27 completed with total reward 0 in 740 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 28 completed with total reward 0 in 358 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 29 completed with total reward 0 in 352 steps\n",
      "----------\n",
      "alpha = 0.3276800000000001\n",
      "Episode 30 completed with total reward 0 in 538 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 31 completed with total reward 0 in 1270 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 32 completed with total reward 0 in 675 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 33 completed with total reward 0 in 336 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 34 completed with total reward 0 in 334 steps\n",
      "----------\n",
      "alpha = 0.2621440000000001\n",
      "Episode 35 completed with total reward 0 in 406 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 36 completed with total reward 0 in 310 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 37 completed with total reward 0 in 322 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 38 completed with total reward 0 in 309 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 39 completed with total reward 0 in 318 steps\n",
      "----------\n",
      "alpha = 0.20971520000000007\n",
      "Episode 40 completed with total reward 0 in 576 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 41 completed with total reward 0 in 335 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 42 completed with total reward 0 in 258 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 43 completed with total reward 0 in 280 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 44 completed with total reward 0 in 313 steps\n",
      "----------\n",
      "alpha = 0.1677721600000001\n",
      "Episode 45 completed with total reward 0 in 255 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 46 completed with total reward 0 in 397 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 47 completed with total reward 0 in 429 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 48 completed with total reward 0 in 245 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 49 completed with total reward 0 in 328 steps\n",
      "----------\n",
      "alpha = 0.13421772800000006\n",
      "Episode 50 completed with total reward 0 in 323 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 51 completed with total reward 0 in 351 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 52 completed with total reward 0 in 303 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 53 completed with total reward 0 in 316 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 54 completed with total reward 0 in 318 steps\n",
      "----------\n",
      "alpha = 0.10737418240000006\n",
      "Episode 55 completed with total reward 0 in 310 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 56 completed with total reward 0 in 342 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 57 completed with total reward 0 in 246 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 58 completed with total reward 0 in 348 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 59 completed with total reward 0 in 251 steps\n",
      "----------\n",
      "alpha = 0.08589934592000005\n",
      "Episode 60 completed with total reward 0 in 268 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 61 completed with total reward 0 in 344 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 62 completed with total reward 0 in 286 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 63 completed with total reward 0 in 277 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 64 completed with total reward 0 in 333 steps\n",
      "----------\n",
      "alpha = 0.06871947673600004\n",
      "Episode 65 completed with total reward 0 in 332 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 66 completed with total reward 0 in 384 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 67 completed with total reward 0 in 247 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 68 completed with total reward 0 in 304 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 69 completed with total reward 0 in 245 steps\n",
      "----------\n",
      "alpha = 0.054975581388800036\n",
      "Episode 70 completed with total reward 0 in 263 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 71 completed with total reward 0 in 301 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 72 completed with total reward 0 in 258 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 73 completed with total reward 0 in 282 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 74 completed with total reward 0 in 226 steps\n",
      "----------\n",
      "alpha = 0.043980465111040035\n",
      "Episode 75 completed with total reward 0 in 241 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 76 completed with total reward 0 in 336 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 77 completed with total reward 0 in 224 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 78 completed with total reward 0 in 250 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 79 completed with total reward 0 in 241 steps\n",
      "----------\n",
      "alpha = 0.03518437208883203\n",
      "Episode 80 completed with total reward 0 in 340 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 81 completed with total reward 0 in 284 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 82 completed with total reward 0 in 271 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 83 completed with total reward 0 in 287 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 84 completed with total reward 0 in 322 steps\n",
      "----------\n",
      "alpha = 0.028147497671065624\n",
      "Episode 85 completed with total reward 0 in 333 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 86 completed with total reward 0 in 241 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 87 completed with total reward 0 in 242 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 88 completed with total reward 0 in 254 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 89 completed with total reward 0 in 314 steps\n",
      "----------\n",
      "alpha = 0.022517998136852502\n",
      "Episode 90 completed with total reward 0 in 247 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 91 completed with total reward 0 in 235 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 92 completed with total reward 0 in 194 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 93 completed with total reward 0 in 281 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 94 completed with total reward 0 in 238 steps\n",
      "----------\n",
      "alpha = 0.018014398509482003\n",
      "Episode 95 completed with total reward 0 in 331 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 96 completed with total reward 0 in 242 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 97 completed with total reward 0 in 229 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 98 completed with total reward 0 in 246 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 99 completed with total reward 0 in 232 steps\n",
      "----------\n",
      "alpha = 0.014411518807585602\n",
      "Episode 100 completed with total reward 0 in 234 steps\n",
      "----------\n",
      "alpha = 0.011529215046068483\n",
      "Episode 101 completed with total reward 0 in 197 steps\n",
      "----------\n",
      "alpha = 0.011529215046068483\n",
      "Episode 102 completed with total reward 0 in 235 steps\n",
      "----------\n",
      "alpha = 0.011529215046068483\n",
      "Episode 103 completed with total reward 0 in 244 steps\n",
      "----------\n",
      "alpha = 0.011529215046068483\n",
      "Episode 104 completed with total reward 0 in 244 steps\n",
      "----------\n",
      "alpha = 0.011529215046068483\n",
      "Episode 105 completed with total reward 0 in 271 steps\n",
      "----------\n",
      "alpha = 0.009223372036854787\n",
      "Episode 106 completed with total reward 0 in 311 steps\n",
      "----------\n",
      "alpha = 0.009223372036854787\n",
      "Episode 107 completed with total reward 0 in 255 steps\n",
      "----------\n",
      "alpha = 0.009223372036854787\n",
      "Episode 108 completed with total reward 0 in 288 steps\n",
      "----------\n",
      "alpha = 0.009223372036854787\n",
      "Episode 109 completed with total reward 0 in 279 steps\n",
      "----------\n",
      "alpha = 0.009223372036854787\n",
      "Episode 110 completed with total reward 0 in 248 steps\n",
      "----------\n",
      "alpha = 0.00737869762948383\n",
      "Episode 111 completed with total reward 0 in 282 steps\n",
      "----------\n",
      "alpha = 0.00737869762948383\n",
      "Episode 112 completed with total reward 0 in 229 steps\n",
      "----------\n",
      "alpha = 0.00737869762948383\n",
      "Episode 113 completed with total reward 0 in 233 steps\n",
      "----------\n",
      "alpha = 0.00737869762948383\n",
      "Episode 114 completed with total reward 0 in 335 steps\n",
      "----------\n",
      "alpha = 0.00737869762948383\n",
      "Episode 115 completed with total reward 0 in 274 steps\n",
      "----------\n",
      "alpha = 0.005902958103587064\n",
      "Episode 116 completed with total reward 0 in 244 steps\n",
      "----------\n",
      "alpha = 0.005902958103587064\n",
      "Episode 117 completed with total reward 0 in 333 steps\n",
      "----------\n",
      "alpha = 0.005902958103587064\n",
      "Episode 118 completed with total reward 0 in 161 steps\n",
      "----------\n",
      "alpha = 0.005902958103587064\n",
      "Episode 119 completed with total reward 0 in 243 steps\n",
      "----------\n",
      "alpha = 0.005902958103587064\n",
      "Episode 120 completed with total reward 0 in 241 steps\n",
      "----------\n",
      "alpha = 0.004722366482869652\n",
      "Episode 121 completed with total reward 0 in 225 steps\n",
      "----------\n",
      "alpha = 0.004722366482869652\n",
      "Episode 122 completed with total reward 0 in 217 steps\n",
      "----------\n",
      "alpha = 0.004722366482869652\n",
      "Episode 123 completed with total reward 0 in 231 steps\n",
      "----------\n",
      "alpha = 0.004722366482869652\n",
      "Episode 124 completed with total reward 0 in 272 steps\n",
      "----------\n",
      "alpha = 0.004722366482869652\n",
      "Episode 125 completed with total reward 0 in 238 steps\n",
      "----------\n",
      "alpha = 0.0037778931862957215\n",
      "Episode 126 completed with total reward 0 in 245 steps\n",
      "----------\n",
      "alpha = 0.0037778931862957215\n",
      "Episode 127 completed with total reward 0 in 327 steps\n",
      "----------\n",
      "alpha = 0.0037778931862957215\n",
      "Episode 128 completed with total reward 0 in 347 steps\n",
      "----------\n",
      "alpha = 0.0037778931862957215\n",
      "Episode 129 completed with total reward 0 in 244 steps\n",
      "----------\n",
      "alpha = 0.0037778931862957215\n",
      "Episode 130 completed with total reward 0 in 285 steps\n",
      "----------\n",
      "alpha = 0.0030223145490365774\n",
      "Episode 131 completed with total reward 0 in 240 steps\n",
      "----------\n",
      "alpha = 0.0030223145490365774\n",
      "Episode 132 completed with total reward 0 in 199 steps\n",
      "----------\n",
      "alpha = 0.0030223145490365774\n",
      "Episode 133 completed with total reward 0 in 236 steps\n",
      "----------\n",
      "alpha = 0.0030223145490365774\n",
      "Episode 134 completed with total reward 0 in 285 steps\n",
      "----------\n",
      "alpha = 0.0030223145490365774\n",
      "Episode 135 completed with total reward 0 in 272 steps\n",
      "----------\n",
      "alpha = 0.002417851639229262\n",
      "Episode 136 completed with total reward 0 in 248 steps\n",
      "----------\n",
      "alpha = 0.002417851639229262\n",
      "Episode 137 completed with total reward 0 in 201 steps\n",
      "----------\n",
      "alpha = 0.002417851639229262\n",
      "Episode 138 completed with total reward 0 in 198 steps\n",
      "----------\n",
      "alpha = 0.002417851639229262\n",
      "Episode 139 completed with total reward 0 in 201 steps\n",
      "----------\n",
      "alpha = 0.002417851639229262\n",
      "Episode 140 completed with total reward 0 in 238 steps\n",
      "----------\n",
      "alpha = 0.0019342813113834097\n",
      "Episode 141 completed with total reward 0 in 341 steps\n",
      "----------\n",
      "alpha = 0.0019342813113834097\n",
      "Episode 142 completed with total reward 0 in 237 steps\n",
      "----------\n",
      "alpha = 0.0019342813113834097\n",
      "Episode 143 completed with total reward 0 in 238 steps\n",
      "----------\n",
      "alpha = 0.0019342813113834097\n",
      "Episode 144 completed with total reward 0 in 202 steps\n",
      "----------\n",
      "alpha = 0.0019342813113834097\n",
      "Episode 145 completed with total reward 0 in 203 steps\n",
      "----------\n",
      "alpha = 0.0015474250491067279\n",
      "Episode 146 completed with total reward 0 in 198 steps\n",
      "----------\n",
      "alpha = 0.0015474250491067279\n",
      "Episode 147 completed with total reward 0 in 235 steps\n",
      "----------\n",
      "alpha = 0.0015474250491067279\n",
      "Episode 148 completed with total reward 0 in 248 steps\n",
      "----------\n",
      "alpha = 0.0015474250491067279\n",
      "Episode 149 completed with total reward 0 in 251 steps\n",
      "----------\n",
      "alpha = 0.0015474250491067279\n",
      "Episode 150 completed with total reward 0 in 236 steps\n"
     ]
    }
   ],
   "source": [
    "q_table = np.zeros((num_states, num_states, env.action_space.n))\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    steps = 0\n",
    "    alpha = max(min_lr, initial_lr * (0.8 ** (episode // 5)))\n",
    "    print('----------')\n",
    "    print(\"alpha = \" + str(alpha))\n",
    "    \n",
    "    while True:\n",
    "        env.render()\n",
    "        pos, vel = discrete_state(env, obs)\n",
    "        \n",
    "        if np.random.uniform(low=0, high=1) < epsilon:\n",
    "            action = np.random.choice(env.action_space.n)\n",
    "        else:\n",
    "            action = np.argmax(q_table[pos][vel])\n",
    "\n",
    "        obs, reward, terminate, _ = env.step(action)\n",
    "        reward = -10 + 3 * abs(obs[0] + 0.5) + 2 * max(obs[0]-0.2,0) + 0.08 * obs[1]\n",
    "\n",
    "        pos_, vel_ = discrete_state(env, obs)\n",
    "        q_table[pos][vel][action] = (1 - alpha) * q_table[pos][vel][action] + alpha * (\n",
    "                reward + gamma * np.max(q_table[pos_][vel_]))\n",
    "        steps += 1\n",
    "        \n",
    "        if terminate:\n",
    "            break\n",
    "    \n",
    "    print(\"Episode {} completed in {} steps\".format(episode + 1, steps))\n",
    "\n",
    "start = time.time()\n",
    "while True:\n",
    "    env.render()\n",
    "    if (time.time() - start) >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_pro03_ai]",
   "language": "python",
   "name": "conda-env-env_pro03_ai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}